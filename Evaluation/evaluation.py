# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H7uYYuQzqR08OGIQ2uJnCy8ORl3qNllq
"""

from collections import defaultdict
import pandas as pd

def evaluate_expanded_metrics(retrieval_df, relevance_df, model_name="model"):
    # Prepare a dictionary to hold results
    results = defaultdict(list)

    # Get all unique query IDs
    query_ids = retrieval_df["query_id"].unique()

    for qid in query_ids:
        # 1. Get all retrieved documents for all variants (variant_id 0-5) of this query
        retrieved_docs = retrieval_df[retrieval_df["query_id"] == qid]["doc_id"].unique()
        retrieved_docs_df = relevance_df[(relevance_df["query_id"] == qid) &
                                         (relevance_df["doc_id"].isin(retrieved_docs))]

        # Expanded Precision:
        #   Denominator = total retrieved docs (after deduplication)
        #   Numerator = number of relevant docs in those
        total_retrieved = len(retrieved_docs)
        relevant_retrieved = retrieved_docs_df["is_relevant"].sum()

        precision = relevant_retrieved / total_retrieved if total_retrieved > 0 else 0

        # Expanded Recall:
        #   Denominator = total number of relevant docs retrieved across all variants
        #   Numerator = number of relevant docs from original query only (variant_id == 0)
        original_variant_docs = retrieval_df[(retrieval_df["query_id"] == qid) &
                                             (retrieval_df["variant_id"] == 0)]["doc_id"].unique()
        original_relevant = relevance_df[(relevance_df["query_id"] == qid) &
                                         (relevance_df["doc_id"].isin(original_variant_docs)) &
                                         (relevance_df["is_relevant"] == 1)].shape[0]

        recall = original_relevant / relevant_retrieved if relevant_retrieved > 0 else 0

        # F1 Score
        if precision + recall > 0:
            f1 = 2 * (precision * recall) / (precision + recall)
        else:
            f1 = 0

        # Save results
        results["query_id"].append(qid)
        results[f"{model_name}_precision"].append(precision)
        results[f"{model_name}_recall"].append(recall)
        results[f"{model_name}_f1"].append(f1)

    return pd.DataFrame(results)

# Load data
retrieval_df = pd.read_csv("Data/hybrid_retrieval_results.csv")
relevance_df = pd.read_csv("Data/hybrid_relevance_labels.csv")

# Display result of hybrid model
hybrid_results_df = evaluate_expanded_metrics(retrieval_df, relevance_df, model_name="hybrid")
hybrid_results_df

# Load data
retrieval_df = pd.read_csv("Data/bm25_retrieval_results.csv")
relevance_df = pd.read_csv("Data/bm25_relevance_labels.csv")

# Display result of bm25 model
bm25_results_df = evaluate_expanded_metrics(retrieval_df, relevance_df, model_name="bm25")
bm25_results_df

# Comparison on average
hybrid_avg = hybrid_results_df[['hybrid_precision', 'hybrid_recall', 'hybrid_f1']].mean()
bm25_avg = bm25_results_df[['bm25_precision', 'bm25_recall', 'bm25_f1']].mean()

average_metrics_df = pd.DataFrame({
    'precision': [hybrid_avg['hybrid_precision'], bm25_avg['bm25_precision']],
    'recall': [hybrid_avg['hybrid_recall'], bm25_avg['bm25_recall']],
    'f1': [hybrid_avg['hybrid_f1'], bm25_avg['bm25_f1']]
}, index=['hybrid', 'bm25'])

average_metrics_df